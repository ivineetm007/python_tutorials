{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Interpreter Lock\n",
    "The Python Global Interpreter Lock or GIL, in simple words, is a mutex (or a lock) that allows only one thread to hold the control of the Python interpreter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Counting\n",
    "It means that objects created in Python have a reference count variable that keeps track of the number of references that point to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "a = []\n",
    "b = a\n",
    "print(sys.getrefcount(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem was that this reference count variable needed protection from race conditions where two threads increase or decrease its value simultaneously.\\\n",
    "### Multiple Locks\n",
    "One way is by adding locks to all data structures that are shared across threads but dding a lock to each object or groups of objects means multiple locks will exist which can cause another problem—Deadlocks (deadlocks can only happen if there is more than one lock). Another side effect would be decreased performance caused by the repeated acquisition and release of locks.\\\n",
    "### Single Lock\n",
    "The GIL is a single lock on the interpreter itself which adds a rule that execution of any Python bytecode requires acquiring the interpreter lock. This prevents deadlocks (as there is only one lock) and doesn’t introduce much performance overhead. But it effectively makes any CPU-bound Python program single-threaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Impact on Multi-Threaded Python Programs\n",
    "A typical python program will either be CPU-bound or I/O bound in their performance.\\\n",
    "**CPU-bound** programs are those that are pushing the CPU to its limit like matrix multiplication, imaghe processing.\\\n",
    "**I/O-bound** programs are the ones that spend time waiting for Input/Output\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds - 7.335408687591553\n"
     ]
    }
   ],
   "source": [
    "# single_threaded.py\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "COUNT = 50000000\n",
    "\n",
    "def countdown(n):\n",
    "    while n>0:\n",
    "        n -= 1\n",
    "\n",
    "start = time.time()\n",
    "countdown(COUNT)\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken in seconds -', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds - 7.502301931381226\n"
     ]
    }
   ],
   "source": [
    "# multi_threaded.py\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "COUNT = 50000000\n",
    "\n",
    "def countdown(n):\n",
    "    while n>0:\n",
    "        n -= 1\n",
    "\n",
    "t1 = Thread(target=countdown, args=(COUNT//2,))\n",
    "t2 = Thread(target=countdown, args=(COUNT//2,))\n",
    "\n",
    "start = time.time()\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken in seconds -', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both versions take almost same amount of time to finish. In the multi-threaded version the GIL prevented the CPU-bound threads from executing in parellel.\\\n",
    "The GIL does not have much impact on the performance of I/O-bound multi-threaded programs as the lock is shared between threads while they are waiting for I/O. But a program whose threads are entirely CPU-bound would not only become single threaded due to the lock but will also see an increase in execution time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement in the GIL\n",
    "We discussed the impact of GIL on “only CPU-bound” and “only I/O-bound” multi-threaded programs but what about the programs where some threads are I/O-bound and some are CPU-bound? In such programs, Python’s GIL was known to starve the I/O-bound threads by not giving them a chance to acquire the GIL from CPU-bound threads.\\\n",
    "This was because of a mechanism built into Python that forced threads to release the GIL after a fixed interval of continuous use and if nobody else acquired the GIL, the same thread could continue its use.\\ \n",
    "The problem in this mechanism was that most of the time the CPU-bound thread would reacquire the GIL itself before other threads could acquire it. This problem was fixed in Python 3.2 in 2009 by Antoine Pitrou who added a mechanism of looking at the number of GIL acquisition requests by other threads that got dropped and not allowing the current thread to reacquire GIL before other threads got a chance to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getswitchinterval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Deal With Python’s GIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-processing vs multi-threading \n",
    "The most popular way is to use a multi-processing approach where you use multiple processes instead of threads. Each Python process gets its own Python interpreter and memory space so the GIL won’t be a problem.\\\n",
    "The below code would not run in the jupyter notebook. Run the multiprocessing_example.py and the other scripts for fair comparison.\n",
    "Below are the results:-\n",
    "1. Single threaded- 3.331\n",
    "2. Multi threaded- 4.614\n",
    "3. Mutliprocessing- 2.6724\n",
    "\n",
    "The time didn’t drop to half of what we saw above because process management has its own overheads. Multiple processes are heavier than multiple threads, so, keep in mind that this could become a scaling bottleneck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# import time\n",
    "\n",
    "# COUNT = 50000000\n",
    "# def countdown(n):\n",
    "#     while n>0:\n",
    "#         n -= 1\n",
    "\n",
    "\n",
    "# pool = Pool(processes=2)\n",
    "# start = time.time()\n",
    "# r1 = pool.apply_async(countdown, [COUNT//2])\n",
    "# r2 = pool.apply_async(countdown, [COUNT//2])\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "# end = time.time()\n",
    "# print('Time taken in seconds -', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. GIL- https://realpython.com/python-gil/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
